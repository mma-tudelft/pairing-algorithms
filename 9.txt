My pairing algorithm makes use of the adjusted cosine equation. The rating vectors are normalized which compensates for users that did not make use of the full scale (basically if they only rated up to 4 stars then it will be stretched as if they rated up to 5 stars). This is to compensate for the cynical movie raters among us. The standard adjusted cosine is then performed making use of the numpy and math libraries to perform the computation. Finally the list of results is processed through a greedy algorithm which matched the best fitting pairs together. I realize this creates a larger gap in correlation between the last pairs selected but in this case (a short movie list and very small user base) I thought it wouldn't compensate to match the last pairs made by greedy with higher correlated partners as this would most likely bring the total correlation sum down for a minor difference. If it wouldn't cost a huge amount of computing power I would have opted for an algorithm that tries all possible combinations and returns the pair set with the highest sum of correlation coefficients.